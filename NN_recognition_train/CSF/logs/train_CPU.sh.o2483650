tools/gcc/git(41):ERROR:105: Unable to locate a modulefile for 'tools/gcc/git/2.8.2'
              total        used        free      shared  buff/cache   available
Mem:         128674        1626      124010          72        3037      126395
Swap:         16383           0       16383
/opt/site/sge/default/spool/node669/job_scripts/2483650: line 14: nvidia-smi: command not found
2021-06-02 09:47:36.932732: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-06-02 09:47:59.359825: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-06-02 09:47:59.364623: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-06-02 09:47:59.364668: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2021-06-02 09:47:59.364717: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node669.pri.csf3.alces.network): /proc/driver/nvidia/version does not exist
2021-06-02 09:47:59.370309: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-02 09:47:59.371755: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-06-02 09:48:00.444346: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-06-02 09:48:00.444882: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599980000 Hz
WARNING:tensorflow:AutoGraph could not transform <function custom_mse at 0x2aee3f39d8b0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
MODEL NAME = step1_default_batchnorm
Epoch 1/10
316/316 - 1233s - loss: 0.6044 - presence_loss: 0.4293 - coordinates_loss: 0.1752 - presence_accuracy: 0.7865
Epoch 2/10
316/316 - 1229s - loss: 0.2301 - presence_loss: 0.1818 - coordinates_loss: 0.0483 - presence_accuracy: 0.9420
Epoch 3/10
316/316 - 1224s - loss: 0.1580 - presence_loss: 0.1322 - coordinates_loss: 0.0259 - presence_accuracy: 0.9559
Epoch 4/10
316/316 - 1224s - loss: 0.1294 - presence_loss: 0.1123 - coordinates_loss: 0.0171 - presence_accuracy: 0.9662
Epoch 5/10
316/316 - 1227s - loss: 0.0913 - presence_loss: 0.0825 - coordinates_loss: 0.0088 - presence_accuracy: 0.9733
Epoch 6/10
316/316 - 1226s - loss: 0.0835 - presence_loss: 0.0736 - coordinates_loss: 0.0100 - presence_accuracy: 0.9769
Epoch 7/10
316/316 - 1229s - loss: 0.0525 - presence_loss: 0.0471 - coordinates_loss: 0.0054 - presence_accuracy: 0.9858
Epoch 8/10
316/316 - 1229s - loss: 0.0594 - presence_loss: 0.0535 - coordinates_loss: 0.0059 - presence_accuracy: 0.9836
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3160 batches). You may need to use the repeat() function when building your dataset.
Epoch 9/10
316/316 - 1346s - loss: 0.0650 - presence_loss: 0.0589 - coordinates_loss: 0.0061 - presence_accuracy: 0.9810
2021-06-02 12:54:11.244027: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
